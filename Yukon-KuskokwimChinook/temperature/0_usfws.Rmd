---
title: "0_usfws"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)
library(lubridate)
library(hms)
library(plotly)
library(tidyverse)

```

# Read in FWS and ARRI data and compare

There are two data sources for USFWS - Office of Subsistence Management data from Meg Perdue that is in their Wiski database and data directly received from Jeff Davis at ARRI (through ADFG data request) for weir sites. These should be mostly the same, but worth checking. Also interest in including Goodnews River from Togiak refuge, which is monitored by Water Resources Branch, not OSM, but I also have from Meg.

USFWS data received from Meg Perdue in October 2020 for AKSSF project. Note that the first sheet is metadata from/for akoats and it indicates some earlier start dates then what Erik and I have in the data inventory. Check the data she sent against the metadata sheet (some sites weren't sent so should be requested) before updating the data inventory.

Note: I am keeping all sites in study area (YK+Norton sound) because I will want YK data for temperature modeling project and possible needs for Erik's idea to map to Chum sites in YK.

```{r osm metadata}
osm_md <- read_excel("S:/Stream Temperature Data/USFWS Perdue - complete 2020/OSM_Oct2020.xlsx", sheet = "OSM_Sites_AKOATS", range = "A1:AP36")

osm_md <- osm_md %>% 
  slice(c(1:22, 25:29, 35)) %>% 
  mutate(Latitude = case_when(is.na(Lat_revised)|Lat_revised == "SAME" ~ Latitude,
                              TRUE ~ as.numeric(Lat_revised)),
         Longitude = case_when(is.na(Long_revised)|Long_revised == "SAME" ~ Longitude,
                              TRUE ~ as.numeric(Long_revised))) %>% 
  select(Agency_ID, SourceName, Latitude, Longitude, Waterbody_name, Initial_date, End_date)

#note we need a SiteID that matches the data for merging
# also need to remove sites that we aren't using (not in study area)

osm_md <- osm_md %>% 
  mutate(SiteID = gsub(" ", "_", paste("OSM", Agency_ID)),
         SiteID = gsub("_Creek", "", SiteID),
         SiteID = gsub("_River", "", SiteID),
         SiteID = gsub("East_Fork", "EF", SiteID)) %>% 
  filter(!grepl("Falls|kdk|Klag|Long|Neva|Newhalen|Telaquana|Tanada|Kanalku", SiteID))
```

One site down low on Middle Fork Goodnews River, three other sites way upstream at lake outlets that we could include to better understand temperatures across both Goodnews proper (north fork) and middle fork. But, these sites are at the very upper limits of Chinook distribution in the watershed.

```{r goodnews river md from togiak}
tog_md <- read_excel("S:/Stream Temperature Data/USFWS Perdue - complete 2020/Togiak_Oct2020_Part2.xlsx", sheet = "AKOATS_metadata", range = "A1:AG15") %>% 
  filter(SiteID =="MFLR") %>% 
  select(Agency_ID = SiteID, SourceName, Latitude, Longitude, Waterbody_name, Initial_date = start_date, End_date) %>% 
  mutate(SiteID = gsub(" ", "_", paste("Togiak", Agency_ID)))

osm_md <- bind_rows(osm_md, tog_md)
```



```{r save osm metadata}
osm_md <- osm_md %>% 
  mutate(chinook_site = case_when(grepl("Sheenjek|Chandalar", Agency_ID) ~ 0,
                                  TRUE ~ 1)) 

saveRDS(osm_md, "data/final_data/osm_md.rds")

```



Read in data sheets.

```{r osm data}
path <- "S:/Stream Temperature Data/USFWS Perdue - complete 2020/OSM_Oct2020.xlsx"

#remove md sheet and also tanada, which did not export correctly
osm_dat <- map_df(excel_sheets(path)[-c(1, 21)], function(x) read_excel(path, sheet = x, na = "---") %>% mutate(sheet = x))

#fix dates and times
osm_dat <- osm_dat %>% 
  # slice(1:100) %>% 
  mutate(sampleDate = as.Date(Date),
         sampleTime = as_hms(format(`Time (UTC -8:00)`, "%H:%M:%S")),
         dt = as.POSIXct(paste(sampleDate, sampleTime, sep = " "), format = "%Y-%m-%d %H:%M:%S"),
         SiteID = paste0("OSM_", sheet),
         sampleYear = year(sampleDate)) %>% 
  select(SiteID, sampleYear, sampleDate, sampleTime, dt, Temperature = 'TW [Â°C]')

#hmmm some have a lot of missing temps, might need to double-check years and make sure data exported correctly from wiski
osm_dat %>% count(SiteID, is.na(Temperature))

#get rid of missing temps and NEW on pitka fork siteid
osm_dat <- osm_dat %>% 
  filter(!is.na(Temperature)) %>% 
  mutate(SiteID = gsub("NEW_", "", SiteID),
         SiteID = gsub("PitkaFork", "Pitka_Fork", SiteID)) 

```

Comparison of data received from Meg against data from Jeff Davis at ARRI:

* mostly Meg's dataset has some more recent years
* missing Aniak - arri has 2008-2011
* Tanada exported incorrectly - but in copper river watershed
* Anvik - arri also has 2009
* Chandalar- arri has ~6 wks from summer 2008, trib of Yukon so keep.
* unalakleet - arri has data from second half of 2012.
* takotna 2008-13 in kuskokwim - bring in from arri.
* sheenjek 2009-12 in yukon so keep.

Sites in ARRI that are not in our study area, some were checked with Jeff's lat/longs:

* afognak - kodiak
* buskin - kodiak 
* gut bay - baranof island, SE
* hatchery - data only from 2013 and no lat/long in arri file
* kanalku - admiralty island, SE
* saltery - kodiak
* stikoh lake - chichigof island, SE
* tanada - upper copper

Meg sites not in study area (note I am keeping everything at this stage even if not a chinook system):

* falls
* kdk sites
* klag lake
* long lake creek
* neva
* newhalen
* telequana
* kanalku creek (SE), and tanada (copper) also in md and removed.


```{r osm site year summary}
osm_dat %>% 
  distinct(SiteID, sampleYear, sampleDate) %>% 
  count(SiteID, sampleYear) %>% 
  arrange(SiteID) %>% 
  pivot_wider(names_from = sampleYear, values_from = n, names_sort = TRUE)

osm_dat %>% 
  distinct(SiteID, sampleYear) %>% 
  group_by(SiteID) %>% 
  arrange(SiteID) %>% 
  summarize(years_lst = paste(sampleYear, collapse = ", "))
```

Plot OSM daily min mean and max for May through September to make sure it looks ok. Filter to remove the sites that we don't need for this project.

* some air temps at Gisasa in July 2019
* very low daily variability in Kwethluk 2012--but hourly sample interval which matches majority of data. Or could be buried, but generally, seasonal variation looks very similar to other years so daily mean should be ok


```{r pdf of dailies for chinook sites, eval = FALSE}

remove_sites <- osm_dat %>% 
  distinct(SiteID) %>% 
  filter(grepl("Falls|kdk|Klag|Long|Neva|Newhalen|Telaquana", SiteID))

osm_dat2 <- osm_dat %>% 
  anti_join(remove_sites)

sites <- osm_dat2 %>% distinct(SiteID)

pdf("output/FWS Sites- Plots of Dailies.pdf", width = 10)
for(i in 1:nrow(sites)) {
  dat <- left_join(sites %>% slice(i), osm_dat2)
  dat2 <- dat %>%
    group_by(SiteID, sampleYear, sampleDate) %>% 
    summarize(meanDT = mean(Temperature),
              minDT = min(Temperature),
              maxDT = max(Temperature)) %>% 
    complete(sampleDate = seq.Date(min(sampleDate), max(sampleDate), by="day")) %>% 
    group_by(sampleYear) %>% 
    mutate(yrct = sum(!is.na(meanDT))) %>% 
    filter(!yrct == 0)
  p1 <- dat2 %>% 
    filter(month(sampleDate) %in% 5:9) %>%
    ggplot() +
    geom_line(aes(x = sampleDate, y= maxDT), color = "red") +
    geom_line(aes(x = sampleDate, y= minDT), color = "blue") +
    geom_line(aes(x = sampleDate, y= meanDT)) +
    facet_wrap(~sampleYear, scales = "free_x") +
    labs(title = sites %>% slice(i) %>% pull(SiteID))
  print(p1)
}
dev.off()


```

Clip Gisasa in 2019 from 7-4 to end.

```{r interactive plot}


p1 <- osm_dat2 %>% 
  filter(SiteID == "OSM_Kwethluk", sampleYear == 2012) %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line() 
  # facet_wrap(~sampleYear, scales = "free_x")

ggplotly(p1)

```


```{r remove bad osm data}
osm_dat2 %>% filter(is.na(dt)) #daylight savings!!!!

#all from dst change
osm_dat2 %>% 
  count(SiteID, dt) %>% filter(n>1)

#sample intervals - majority are hourly
osm_dat2 %>% 
  count(SiteID, sampleDate) %>%
  count(n) %>% 
  arrange(n)


osm_dat3 <- osm_dat2 %>% 
  filter(!(SiteID == "OSM_Gisasa" & sampleDate > as.Date("2019-07-04")),
         !is.na(dt)) 

#634 for gisasa + 82 for dst change
nrow(osm_dat2) - nrow(osm_dat3) #good

```

Just saving daily data, that is what is from ARRI and USGS and what I saved for Al's data in Canada.

```{r save raw OSM data, eval = FALSE}
# saveRDS(osm_dat3, "data/final_data/osm_dat.rds")

```

Convert raw OSM data from Meg to dailies and combine with ARRI to make a complete dataset.

```{r create fws daily file}
osm_daily <- osm_dat3 %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanDT = mean(Temperature),
            maxDT = max(Temperature),
            minDT = min(Temperature))

```

Compared Meg's data summary with data inventory:

* bring in Aniak and Takotna from ARRI. Note that the years are on separate sheets and all start on different cells, will have to bring in manually and combine.
* bring in Anvik 2009 and Unalakleet 2012 to get complete set of data

```{r arri data}
arri.folder <- "S:\\Stream Temperature Data\\Jeff Davis - FWS weir data\\FWS Temp Site Summaries 2008 - 2020"

path <- paste0(arri.folder, "\\Takotna.xlsx")
takotna <- map_df(excel_sheets(path)[-c(1)], function(x) read_excel(path, sheet = x, range = "A24:D400") %>% 
                    mutate(SiteID = "OSM_Takotna") %>% filter(!is.na(Ave)))

path <- paste0(arri.folder, "\\Aniak.xlsx")
aniak <- map_df(excel_sheets(path)[-c(1)], function(x) read_excel(path, sheet = x, range = "A27:D400", col_names = c("Date", "Max", "Min", "Ave")) %>% 
                  mutate(SiteID = "OSM_Aniak") %>% filter(!is.na(Ave)))

path <- paste0(arri.folder, "\\Sheenjek.xlsx")
sheenjek <- map_df(excel_sheets(path)[c(3:6)], function(x) read_excel(path, sheet = x, range = "A25:D400", col_names = c("Date", "Max", "Min", "Ave")) %>% 
                  mutate(SiteID = "OSM_Sheenjek") %>% filter(!is.na(Ave)))

anvik09 <- read_excel(paste0(arri.folder, "\\Anvik.xlsx"), sheet = "Anvik 2009", range = "A26:D400") %>% 
  mutate(SiteID = "OSM_Anvik") %>% filter(!is.na(Ave))
unalk12 <- read_excel(paste0(arri.folder, "\\Unalakleet.xlsx"), sheet = "Unalakleet 2012", range = "A24:D400") %>% 
  mutate(SiteID = "OSM_Unalakleet") %>% filter(!is.na(Ave))

arri_dat <- bind_rows(takotna, aniak, sheenjek, anvik09, unalk12) %>% 
  mutate(sampleDate = as.Date(Date),
         meanDT = Ave,
         maxDT = Max,
         minDT = Min) %>% 
  select(SiteID, sampleDate, meanDT, maxDT, minDT)

arri_dat %>% summary()

```

Plot daily arri data that we are using to check that it all looks good.

```{r pdf of dailies for arri data, eval = FALSE}

sites <- arri_dat %>% distinct(SiteID)

pdf("output/ARRI data - Plots of Dailies.pdf", width = 10)
for(i in 1:nrow(sites)) {
  dat <- left_join(sites %>% slice(i), arri_dat)
  dat2 <- dat %>%
    mutate(sampleYear = year(sampleDate)) %>% 
    complete(sampleDate = seq.Date(min(sampleDate), max(sampleDate), by="day")) %>% 
    group_by(sampleYear) %>% 
    mutate(yrct = sum(!is.na(meanDT))) %>% 
    filter(!yrct == 0)
  p1 <- dat2 %>% 
    filter(month(sampleDate) %in% 5:9) %>%
    ggplot() +
    geom_line(aes(x = sampleDate, y= maxDT), color = "red") +
    geom_line(aes(x = sampleDate, y= minDT), color = "blue") +
    geom_line(aes(x = sampleDate, y= meanDT)) +
    facet_wrap(~sampleYear, scales = "free_x") +
    labs(title = sites %>% slice(i) %>% pull(SiteID))
  print(p1)
}
dev.off()


```

```{r MFLR data}

tog_dat <- read_excel("S:/Stream Temperature Data/USFWS Perdue - complete 2020/Togiak_Oct2020_Part2.xlsx", sheet = "MFLR")  

tog_daily <- tog_dat %>% 
  mutate(sampleDate = as.Date(Date),
         SiteID = "Togiak_MFLR",
         Temperature = as.numeric(`TW [Â°C]`)) %>% 
  filter(!is.na(Temperature)) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanDT = mean(Temperature),
         maxDT = max(Temperature),
         minDT = min(Temperature)) 
  

tog_daily %>%
    mutate(sampleYear = year(sampleDate)) %>% 
    complete(sampleDate = seq.Date(min(sampleDate), max(sampleDate), by="day")) %>% 
    group_by(sampleYear) %>% 
    mutate(yrct = sum(!is.na(meanDT))) %>% 
    filter(!yrct == 0) %>% 
    filter(month(sampleDate) %in% 5:9) %>%
    ggplot() +
    geom_line(aes(x = sampleDate, y= maxDT), color = "red") +
    geom_line(aes(x = sampleDate, y= minDT), color = "blue") +
    geom_line(aes(x = sampleDate, y= meanDT)) +
    facet_wrap(~sampleYear, scales = "free_x") 

```



Create complete dataset for USFWS weirs - data combined from USFWS and ARRI. Add in togiak data as well.


```{r complete osm dataset}
#strange duplicate in arri data remove before binding
bind_rows(arri_dat, osm_daily, tog_daily) %>% 
  count(SiteID, sampleDate) %>% 
  filter(n > 1)

arri_dat[duplicated(arri_dat),]

osm_daily2 <- bind_rows(arri_dat[!duplicated(arri_dat),], osm_daily, tog_daily) %>% 
  mutate(sampleYear = year(sampleDate))


saveRDS(osm_daily2, "data/final_data/osm_dat.rds")

```

Summary to update online.

```{r data summary}
osm_daily2 %>% 
  distinct(SiteID, sampleYear, sampleDate) %>% 
  count(SiteID, sampleYear) %>% 
  pivot_wider(names_from = sampleYear, values_from = n, names_sort = TRUE)

osm_daily2 %>% 
  distinct(SiteID, sampleYear) %>% 
  group_by(SiteID) %>% 
  arrange(SiteID, sampleYear) %>% 
  summarize(years_lst = paste(sampleYear, collapse = ", ")) %>% 
  left_join(osm_md %>% select(SiteID, Latitude, Longitude)) %>% 
  write_csv("data/osm_data_summ.csv")
```

